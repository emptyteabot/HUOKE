"""
ç»Ÿä¸€çˆ¬è™«è°ƒåº¦å™¨ - å¤šå¹³å°å¹¶è¡ŒæŠ“å–

æ”¯æŒå¹³å°:
1. å°çº¢ä¹¦
2. çŸ¥ä¹
3. å¾®åš

åŠŸèƒ½:
- å¤šå¹³å°å¹¶è¡ŒæŠ“å–
- ç»Ÿä¸€æ•°æ®æ ¼å¼
- è‡ªåŠ¨å»é‡åˆå¹¶
- è¿›åº¦æ˜¾ç¤º
- é”™è¯¯é‡è¯•
- å¯¼å‡ºExcelæŠ¥å‘Š
"""

import time
import random
import json
import pandas as pd
from datetime import datetime
from typing import List, Dict, Set
import os
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from pathlib import Path

# å¯¼å…¥Cookieç®¡ç†å™¨
from cookie_manager import CookieManager

try:
    import undetected_chromedriver as uc
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("âŒ è¯·å…ˆå®‰è£…ä¾èµ–: pip install selenium undetected-chromedriver pandas openpyxl")
    sys.exit(1)


class ScraperManager:
    """ç»Ÿä¸€çˆ¬è™«è°ƒåº¦å™¨"""

    def __init__(self, account: str = "default"):
        self.results = []
        self.results_lock = Lock()
        self.seen_users = set()  # å»é‡ç”¨
        self.platform_stats = {}
        self.account = account
        self.cookie_manager = CookieManager()

        # å¹³å°é…ç½®
        self.platform_config = {
            'xiaohongshu': {
                'name': 'å°çº¢ä¹¦',
                'url': 'https://www.xiaohongshu.com',
                'search_url': 'https://www.xiaohongshu.com/search_result?keyword={keyword}',
                'login_check': '.avatar, .user-avatar',
                'enabled': True
            },
            'zhihu': {
                'name': 'çŸ¥ä¹',
                'url': 'https://www.zhihu.com',
                'search_url': 'https://www.zhihu.com/search?type=content&q={keyword}',
                'login_check': '.Avatar',
                'enabled': True
            },
            'weibo': {
                'name': 'å¾®åš',
                'url': 'https://weibo.com',
                'search_url': 'https://s.weibo.com/weibo?q={keyword}',
                'login_check': '.Avatar_face',
                'enabled': True
            }
        }

    def init_driver(self, headless: bool = False):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        options = uc.ChromeOptions()
        if headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')

        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]
        options.add_argument(f'user-agent={random.choice(user_agents)}')

        driver = uc.Chrome(options=options)
        driver.set_page_load_timeout(30)
        driver.maximize_window()

        return driver

    def random_sleep(self, min_sec: float = 2, max_sec: float = 5):
        """éšæœºå»¶è¿Ÿ"""
        time.sleep(random.uniform(min_sec, max_sec))

    def login_platform(self, driver, platform: str) -> bool:
        """ç™»å½•å¹³å° - æ”¯æŒCookieè‡ªåŠ¨ç™»å½•"""
        config = self.platform_config[platform]

        print(f"\n{'='*50}")
        print(f"ğŸ” ç™»å½•{config['name']}")
        print("="*50)

        # å°è¯•ä½¿ç”¨Cookieç™»å½•
        print("æ£€æŸ¥å·²ä¿å­˜çš„Cookie...")
        if self.cookie_manager.is_valid(driver, platform, self.account):
            print(f"âœ… {config['name']} Cookieæœ‰æ•ˆ,è‡ªåŠ¨ç™»å½•æˆåŠŸ!")
            return True

        print("Cookieæ— æ•ˆæˆ–ä¸å­˜åœ¨,éœ€è¦æ‰‹åŠ¨ç™»å½•")

        driver.get(config['url'])
        time.sleep(3)

        print(f"\nè¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•{config['name']}:")
        print("1. æ‰«ç ç™»å½• æˆ– æ‰‹æœºå·ç™»å½•")
        print("2. ç™»å½•æˆåŠŸå,è¾“å…¥ 'ok' å¹¶æŒ‰å›è½¦...")

        while True:
            user_input = input("\nè¾“å…¥ 'ok' ç»§ç»­: ").strip().lower()
            if user_input == 'ok':
                break

        try:
            driver.find_element(By.CSS_SELECTOR, config['login_check'])
            print(f"âœ… {config['name']}ç™»å½•æˆåŠŸ!")

            # ä¿å­˜Cookie
            cookies = driver.get_cookies()
            if self.cookie_manager.save_cookies(platform, self.account, cookies):
                print(f"âœ… Cookieå·²ä¿å­˜,ä¸‹æ¬¡å°†è‡ªåŠ¨ç™»å½•")

            return True
        except:
            print(f"âš ï¸ æœªæ£€æµ‹åˆ°ç™»å½•,ç»§ç»­å°è¯•...")
            return True

    def scrape_xiaohongshu(self, keyword: str, limit: int, max_retries: int = 3) -> List[Dict]:
        """çˆ¬å–å°çº¢ä¹¦"""
        platform = 'xiaohongshu'
        results = []
        driver = None

        for attempt in range(max_retries):
            try:
                print(f"\nğŸ“± [{self.platform_config[platform]['name']}] å¼€å§‹çˆ¬å– (å°è¯• {attempt+1}/{max_retries})")

                driver = self.init_driver()

                if not self.login_platform(driver, platform):
                    continue

                # æœç´¢
                search_url = self.platform_config[platform]['search_url'].format(keyword=keyword)
                driver.get(search_url)
                self.random_sleep(3, 5)

                # æ»šåŠ¨åŠ è½½
                for i in range(3):
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    self.random_sleep(2, 3)

                # è·å–ç¬”è®°é“¾æ¥
                note_links = []
                note_elements = driver.find_elements(By.CSS_SELECTOR, "a[href*='/explore/']")

                for elem in note_elements[:limit]:
                    try:
                        link = elem.get_attribute("href")
                        if link and link not in note_links:
                            note_links.append(link)
                    except:
                        continue

                print(f"  æ‰¾åˆ° {len(note_links)} ä¸ªç¬”è®°")

                # çˆ¬å–è¯„è®º
                for idx, note_url in enumerate(note_links):
                    print(f"  è¿›åº¦: {idx+1}/{len(note_links)}")

                    driver.get(note_url)
                    self.random_sleep(3, 5)

                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    self.random_sleep(2, 3)

                    comment_elements = driver.find_elements(By.CSS_SELECTOR, ".comment-item, [class*='comment']")

                    for comment_elem in comment_elements[:20]:
                        try:
                            username = comment_elem.find_element(By.CSS_SELECTOR, ".username, [class*='username']").text.strip()
                            content = comment_elem.find_element(By.CSS_SELECTOR, ".content, [class*='content']").text.strip()

                            if username and content:
                                results.append({
                                    'platform': 'å°çº¢ä¹¦',
                                    'username': username,
                                    'content': content,
                                    'source_url': note_url,
                                    'scraped_at': datetime.now().isoformat()
                                })
                        except:
                            continue

                    self.random_sleep(3, 6)

                print(f"âœ… [{self.platform_config[platform]['name']}] æˆåŠŸçˆ¬å– {len(results)} æ¡æ•°æ®")
                break

            except Exception as e:
                print(f"âŒ [{self.platform_config[platform]['name']}] çˆ¬å–å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    print(f"âŒ [{self.platform_config[platform]['name']}] è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°,æ”¾å¼ƒ")

            finally:
                if driver:
                    driver.quit()

        return results

    def scrape_zhihu(self, keyword: str, limit: int, max_retries: int = 3) -> List[Dict]:
        """çˆ¬å–çŸ¥ä¹"""
        platform = 'zhihu'
        results = []
        driver = None

        for attempt in range(max_retries):
            try:
                print(f"\nğŸ“š [{self.platform_config[platform]['name']}] å¼€å§‹çˆ¬å– (å°è¯• {attempt+1}/{max_retries})")

                driver = self.init_driver()

                if not self.login_platform(driver, platform):
                    continue

                # æœç´¢
                search_url = self.platform_config[platform]['search_url'].format(keyword=keyword)
                driver.get(search_url)
                self.random_sleep(3, 5)

                # æ»šåŠ¨åŠ è½½
                for i in range(3):
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    self.random_sleep(2, 3)

                # è·å–å†…å®¹é“¾æ¥
                content_links = []
                link_elements = driver.find_elements(By.CSS_SELECTOR, "a[href*='/question/'], a[href*='/answer/']")

                for elem in link_elements[:limit]:
                    try:
                        link = elem.get_attribute("href")
                        if link and link not in content_links:
                            content_links.append(link)
                    except:
                        continue

                print(f"  æ‰¾åˆ° {len(content_links)} ä¸ªå†…å®¹")

                # çˆ¬å–è¯„è®º
                for idx, content_url in enumerate(content_links):
                    print(f"  è¿›åº¦: {idx+1}/{len(content_links)}")

                    driver.get(content_url)
                    self.random_sleep(3, 5)

                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    self.random_sleep(2, 3)

                    comment_elements = driver.find_elements(By.CSS_SELECTOR, ".CommentItem, [class*='Comment']")

                    for comment_elem in comment_elements[:20]:
                        try:
                            username = comment_elem.find_element(By.CSS_SELECTOR, ".UserLink, [class*='UserLink']").text.strip()
                            content = comment_elem.find_element(By.CSS_SELECTOR, ".CommentContent, [class*='Content']").text.strip()

                            if username and content:
                                results.append({
                                    'platform': 'çŸ¥ä¹',
                                    'username': username,
                                    'content': content,
                                    'source_url': content_url,
                                    'scraped_at': datetime.now().isoformat()
                                })
                        except:
                            continue

                    self.random_sleep(3, 6)

                print(f"âœ… [{self.platform_config[platform]['name']}] æˆåŠŸçˆ¬å– {len(results)} æ¡æ•°æ®")
                break

            except Exception as e:
                print(f"âŒ [{self.platform_config[platform]['name']}] çˆ¬å–å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    print(f"âŒ [{self.platform_config[platform]['name']}] è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°,æ”¾å¼ƒ")

            finally:
                if driver:
                    driver.quit()

        return results

    def scrape_weibo(self, keyword: str, limit: int, max_retries: int = 3) -> List[Dict]:
        """çˆ¬å–å¾®åš"""
        platform = 'weibo'
        results = []
        driver = None

        for attempt in range(max_retries):
            try:
                print(f"\nğŸ¦ [{self.platform_config[platform]['name']}] å¼€å§‹çˆ¬å– (å°è¯• {attempt+1}/{max_retries})")

                driver = self.init_driver()

                if not self.login_platform(driver, platform):
                    continue

                # æœç´¢
                search_url = self.platform_config[platform]['search_url'].format(keyword=keyword)
                driver.get(search_url)
                self.random_sleep(3, 5)

                # æ»šåŠ¨åŠ è½½
                for i in range(3):
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    self.random_sleep(2, 3)

                # è·å–å¾®åš
                weibo_elements = driver.find_elements(By.CSS_SELECTOR, ".card-wrap, [class*='card']")[:limit]

                print(f"  æ‰¾åˆ° {len(weibo_elements)} æ¡å¾®åš")

                for idx, weibo_elem in enumerate(weibo_elements):
                    print(f"  è¿›åº¦: {idx+1}/{len(weibo_elements)}")

                    try:
                        comment_btn = weibo_elem.find_element(By.CSS_SELECTOR, "[action-type='feed_list_comment']")
                        comment_btn.click()
                        self.random_sleep(2, 3)

                        comment_elements = driver.find_elements(By.CSS_SELECTOR, ".list_li, [class*='comment']")

                        for comment_elem in comment_elements[:10]:
                            try:
                                username = comment_elem.find_element(By.CSS_SELECTOR, ".name, [class*='name']").text.strip()
                                content = comment_elem.find_element(By.CSS_SELECTOR, ".txt, [class*='text']").text.strip()

                                if username and content:
                                    results.append({
                                        'platform': 'å¾®åš',
                                        'username': username,
                                        'content': content,
                                        'source_url': driver.current_url,
                                        'scraped_at': datetime.now().isoformat()
                                    })
                            except:
                                continue
                    except:
                        continue

                    self.random_sleep(3, 6)

                print(f"âœ… [{self.platform_config[platform]['name']}] æˆåŠŸçˆ¬å– {len(results)} æ¡æ•°æ®")
                break

            except Exception as e:
                print(f"âŒ [{self.platform_config[platform]['name']}] çˆ¬å–å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    print(f"âŒ [{self.platform_config[platform]['name']}] è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°,æ”¾å¼ƒ")

            finally:
                if driver:
                    driver.quit()

        return results

    def scrape_platform(self, platform: str, keyword: str, limit: int) -> List[Dict]:
        """è°ƒåº¦å•ä¸ªå¹³å°çˆ¬è™«"""
        if platform == 'xiaohongshu':
            return self.scrape_xiaohongshu(keyword, limit)
        elif platform == 'zhihu':
            return self.scrape_zhihu(keyword, limit)
        elif platform == 'weibo':
            return self.scrape_weibo(keyword, limit)
        else:
            print(f"âŒ ä¸æ”¯æŒçš„å¹³å°: {platform}")
            return []

    def deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """å»é‡"""
        print(f"\nğŸ”„ æ•°æ®å»é‡...")

        seen = set()
        unique_results = []

        for item in results:
            key = (item['platform'], item['username'], item['content'])
            if key not in seen:
                seen.add(key)
                unique_results.append(item)

        removed = len(results) - len(unique_results)
        print(f"âœ… å»é‡å®Œæˆ: ç§»é™¤ {removed} æ¡é‡å¤æ•°æ®")

        return unique_results

    def filter_high_intent(self, results: List[Dict]) -> List[Dict]:
        """ç­›é€‰é«˜æ„å‘ç”¨æˆ·"""
        print(f"\nğŸ¯ ç­›é€‰é«˜æ„å‘ç”¨æˆ·...")

        intent_keywords = [
            "æƒ³å’¨è¯¢", "æ±‚æ¨è", "æ€ä¹ˆç”³è¯·", "æœ‰æ²¡æœ‰", "æ±‚è”ç³»",
            "åŠ å¾®ä¿¡", "ç§ä¿¡", "æ±‚åŠ©", "è¯·é—®", "äº†è§£ä¸€ä¸‹",
            "æƒ³å»", "æ‰“ç®—", "å‡†å¤‡", "è€ƒè™‘", "æœ‰æ„å‘",
            "æ±‚ä»‹ç»", "æ±‚åˆ†äº«", "æƒ³çŸ¥é“", "æ±‚é—®", "æœ‰äººçŸ¥é“å—"
        ]

        high_intent = []

        for item in results:
            content = item['content'].lower()

            if any(keyword in content for keyword in intent_keywords):
                item['intent_level'] = 'high'
                high_intent.append(item)
            else:
                item['intent_level'] = 'low'

        print(f"âœ… æ‰¾åˆ° {len(high_intent)} ä¸ªé«˜æ„å‘ç”¨æˆ·")

        return high_intent

    def save_to_excel(self, results: List[Dict], filename: str):
        """ä¿å­˜åˆ°Excel"""
        print(f"\nğŸ’¾ ä¿å­˜åˆ°Excel: {filename}")

        try:
            df = pd.DataFrame(results)

            columns = ['platform', 'username', 'content', 'intent_level', 'source_url', 'scraped_at']
            df = df[columns]

            df.to_excel(filename, index=False, engine='openpyxl')

            print(f"âœ… æˆåŠŸä¿å­˜ {len(results)} æ¡æ•°æ®")
            print(f"ğŸ“‚ æ–‡ä»¶ä½ç½®: {os.path.abspath(filename)}")

        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")

    def generate_report(self, results: List[Dict], high_intent: List[Dict]):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        print(f"\n{'='*50}")
        print("ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š")
        print("="*50)

        # æŒ‰å¹³å°ç»Ÿè®¡
        platform_counts = {}
        for item in results:
            platform = item['platform']
            platform_counts[platform] = platform_counts.get(platform, 0) + 1

        print("\nå„å¹³å°æ•°æ®é‡:")
        for platform, count in platform_counts.items():
            print(f"  {platform}: {count} æ¡")

        print(f"\næ€»æ•°æ®é‡: {len(results)} æ¡")
        print(f"é«˜æ„å‘ç”¨æˆ·: {len(high_intent)} æ¡")
        print(f"é«˜æ„å‘å æ¯”: {len(high_intent)/len(results)*100:.1f}%")

    def run(self, platforms: List[str], keyword: str, limit: int = 10):
        """è¿è¡Œå¤šå¹³å°çˆ¬è™«"""
        print(f"\n{'='*50}")
        print("ğŸš€ ç»Ÿä¸€çˆ¬è™«è°ƒåº¦å™¨")
        print("="*50)
        print(f"å…³é”®è¯: {keyword}")
        print(f"å¹³å°: {', '.join([self.platform_config[p]['name'] for p in platforms])}")
        print(f"æ¯å¹³å°æ•°é‡: {limit}")

        all_results = []

        # ä¸²è¡Œæ‰§è¡Œ(é¿å…æµè§ˆå™¨å†²çª)
        for platform in platforms:
            if platform in self.platform_config:
                results = self.scrape_platform(platform, keyword, limit)
                all_results.extend(results)

                # ä¿å­˜ä¸­é—´ç»“æœ
                with self.results_lock:
                    self.results.extend(results)

        if not all_results:
            print("\nâŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            return

        # å»é‡
        unique_results = self.deduplicate_results(all_results)

        # ç­›é€‰é«˜æ„å‘
        high_intent = self.filter_high_intent(unique_results)

        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # ä¿å­˜å…¨éƒ¨æ•°æ®
        all_filename = f"multi_platform_all_{timestamp}.xlsx"
        self.save_to_excel(unique_results, all_filename)

        # ä¿å­˜é«˜æ„å‘æ•°æ®
        if high_intent:
            high_intent_filename = f"multi_platform_high_intent_{timestamp}.xlsx"
            self.save_to_excel(high_intent, high_intent_filename)

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(unique_results, high_intent)

        print(f"\n{'='*50}")
        print("ğŸ‰ å®Œæˆ!")
        print("="*50)


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*50)
    print("ğŸ¯ ç»Ÿä¸€çˆ¬è™«è°ƒåº¦å™¨")
    print("="*50)

    print("\næ”¯æŒçš„å¹³å°:")
    print("1. xiaohongshu - å°çº¢ä¹¦")
    print("2. zhihu - çŸ¥ä¹")
    print("3. weibo - å¾®åš")

    # é€‰æ‹©å¹³å°
    platform_input = input("\nè¯·é€‰æ‹©å¹³å° (å¤šä¸ªç”¨é€—å·åˆ†éš”, ä¾‹å¦‚: xiaohongshu,zhihu): ").strip().lower()
    platforms = [p.strip() for p in platform_input.split(',')]

    # éªŒè¯å¹³å°
    valid_platforms = ['xiaohongshu', 'zhihu', 'weibo']
    platforms = [p for p in platforms if p in valid_platforms]

    if not platforms:
        print("âŒ æœªé€‰æ‹©æœ‰æ•ˆå¹³å°")
        return

    # è¾“å…¥å…³é”®è¯
    keyword = input("\nè¯·è¾“å…¥æœç´¢å…³é”®è¯ (ä¾‹å¦‚: ç¾å›½ç•™å­¦): ").strip()
    if not keyword:
        keyword = "ç¾å›½ç•™å­¦"

    # è¾“å…¥æ•°é‡
    limit = input("\nè¯·è¾“å…¥æ¯ä¸ªå¹³å°è¦çˆ¬å–çš„å†…å®¹æ•°é‡ (é»˜è®¤10): ").strip()
    limit = int(limit) if limit else 10

    # è¿è¡Œ
    manager = ScraperManager()
    manager.run(platforms, keyword, limit)


if __name__ == "__main__":
    main()
