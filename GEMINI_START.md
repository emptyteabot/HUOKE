# 🚀 Gemini启动Prompt - 复制粘贴即用

---

## 第一次对话 - 让Gemini理解项目

```
你好!我需要你作为我的技术合伙人,帮我完成一个留学获客系统项目。

项目GitHub: https://github.com/emptyteabot/HUOKE

请按以下步骤执行:

## 第1步: 理解项目

请阅读以下文件:
1. README.md - 项目总览
2. GEMINI_CONTEXT.md - 当前状态和核心问题
3. GEMINI_WORKFLOW.md - 完整工作流程
4. Gemini获客Prompt工程.md - 数据收集Prompt模板

## 第2步: 生成分析报告

请生成以下内容:

### 2.1 项目架构分析
- 核心模块有哪些?
- 技术栈是什么?
- 数据流向如何?
- 有哪些依赖?

### 2.2 当前问题清单
- 代码质量问题(按严重程度排序)
- 功能缺失
- 性能瓶颈
- 安全隐患

### 2.3 30天开发计划
按优先级列出:
- Week 1: [任务列表]
- Week 2: [任务列表]
- Week 3: [任务列表]
- Week 4: [任务列表]

### 2.4 今日任务清单
列出今天应该完成的TOP 3任务,包括:
- 任务描述
- 预计时间
- 优先级
- 依赖关系

## 第3步: 提出建议

基于你的分析,提出:
1. 最紧急需要解决的问题(TOP 3)
2. 快速见效的优化方向(Quick Wins)
3. 长期架构改进建议

请以Markdown格式输出完整报告。

开始!
```

---

## 第二次对话 - 数据收集(最优先)

```
很好!现在开始第一个任务:收集真实客户数据。

这是整个项目最关键的一步,因为没有真实数据,其他功能都是空中楼阁。

## 任务: 收集150-200个真实留学潜在客户

请使用项目中的 `Gemini获客Prompt工程.md` 作为参考,执行以下步骤:

### 步骤1: 找到客户聚集地
搜索并列出中国学生讨论留学申请的TOP 20平台:
- 小红书 (xiaohongshu.com)
- 知乎 (zhihu.com)
- 豆瓣小组 (douban.com)
- 微博 (weibo.com)
- 留学论坛(寄托天下、一亩三分地等)

对每个平台,提供:
- 平台名称和URL
- 活跃度(高/中/低)
- 是否容易获取联系方式
- 推荐的搜索关键词(至少5个)

### 步骤2: 搜索高价值内容
在这些平台搜索以下关键词:
- "留学中介推荐"
- "英国留学申请"
- "美国研究生DIY"
- "留学机构避坑"
- "求推荐靠谱中介"
- "留学申请求助"

找到最近30天内的热门帖子/问题(点赞>100或评论>50)

### 步骤3: 提取用户信息
对于每个发帖/评论的用户,提取:

**基本信息:**
- 用户名/昵称
- 平台
- 用户主页链接

**联系方式(如果公开):**
- 邮箱
- 微信号
- QQ号
- 手机号

**背景信息(从内容推断):**
- 本科学校
- 专业
- GPA/成绩
- 年级

**目标信息:**
- 目标国家
- 目标学校
- 目标专业
- 学位(本科/硕士/博士)

**意向分析:**
- 预算范围
- 申请时间线
- 痛点(2-3个)
- 行为信号(搜索/咨询/对比等)

**评分:**
- 意向评分(1-10分,综合考虑:发帖频率、提问具体度、时间紧迫度、预算匹配度)
- 优先级(S/A/B级)

### 步骤4: 输出标准化数据

以JSON格式输出,每个客户包含以下字段:

```json
{
  "id": "唯一ID",
  "name": "张同学",
  "platform": "小红书",
  "profile_url": "https://...",
  "contact": {
    "email": "zhang@qq.com",
    "wechat": "zhang123",
    "phone": "13812345678",
    "qq": "123456789"
  },
  "background": {
    "school": "某985大学",
    "major": "计算机科学",
    "gpa": "3.5",
    "grade": "大三"
  },
  "target": {
    "country": "英国",
    "university": "UCL",
    "major": "CS",
    "degree": "硕士"
  },
  "budget": "30-50万",
  "timeline": "2024年9月入学",
  "pain_points": [
    "不知道如何选校",
    "文书不会写",
    "担心DIY失败"
  ],
  "signals": [
    "在小红书搜索了'UCL申请要求'",
    "评论区咨询中介推荐",
    "最近7天内发了3条相关帖子"
  ],
  "intent_score": 8,
  "priority": "S",
  "source_url": "https://...",
  "collected_at": "2024-02-22 10:30:00",
  "notes": "非常紧迫,申请季临近"
}
```

### 目标:
- 小红书: 50个客户
- 知乎: 30个客户
- 豆瓣: 20个客户
- 微博: 25个客户
- 论坛: 40个客户
- **总计: 165个客户**

### 质量要求:
- 必须是真实用户(不是AI生成)
- 联系方式完整率 >60%
- 意向评分>7的占比 >60%
- 数据来源可追溯

### 输出:
1. gemini_leads.json (完整数据)
2. data_collection_report.md (收集报告,包括:数据来源、质量分析、意向分布、联系方式完整率)

开始执行!
```

---

## 第三次对话 - 数据验证

```
太好了!我收到了你提供的数据。

现在需要验证和清洗数据。

## 任务: 数据质量验证和清洗

### 步骤1: 数据质量检查

请检查 gemini_leads.json,生成质量报告:

**完整性检查:**
- 总数据量: X条
- 必填字段完整率:
  * name: X%
  * platform: X%
  * contact(至少一项): X%
  * intent_score: X%

**格式验证:**
- 邮箱格式正确: X条
- 手机号格式正确: X条
- URL可访问: X条

**数据分布:**
- 平台分布: 小红书X条, 知乎X条...
- 意向分布: S级X条, A级X条, B级X条
- 评分分布: 9-10分X条, 7-8分X条...

**问题数据:**
- 缺少联系方式: X条
- 信息不完整: X条
- 疑似重复: X条

### 步骤2: 数据清洗

请执行以下清洗操作:

**去重:**
- 检测同一个人在多个平台出现
- 合并重复数据,保留信息最完整的
- 标注数据来源(多个平台)

**标准化:**
- 统一字段格式
- 补充缺失信息(从其他字段推断)
- 修正明显错误

**增强:**
- 根据行为信号重新评分
- 生成个性化触达话术
- 推荐最佳联系时间和渠道

### 步骤3: 输出清洗后数据

生成以下文件:

1. **gemini_leads_cleaned.json** (清洗后的数据)
2. **data_quality_report.md** (质量报告)
   - 原始数据: X条
   - 去重后: X条
   - 有效数据: X条
   - 联系方式完整率: X%
   - S/A/B级分布
   - 问题数据列表

3. **outreach_plan.md** (触达计划)
   - S级客户(立即联系): X条
   - A级客户(3天内): X条
   - B级客户(1周内): X条
   - 每个客户的触达策略

开始执行!
```

---

## 第四次对话 - 代码重构

```
数据收集完成!现在优化代码质量。

## 任务: 重构爬虫代码

项目中有两个爬虫文件存在严重问题:
- scrapers/xiaohongshu_scraper_v2.py
- scrapers/zhihu_scraper_v2.py

### 问题分析:
1. 大量使用 time.sleep() 硬等待 → 效率低
2. 用 except Exception: continue 掩盖错误 → 无法调试
3. 代码高度冗余 → 难以维护
4. 没有重试机制 → 容易失败
5. 选择器写死 → 平台更新就失效

### 已完成:
- ✅ scrapers/base_scraper.py (基类)
- ✅ scrapers/xiaohongshu_scraper_pro.py (重构示例)

### 需要你做:

#### 任务1: 重构知乎爬虫
基于 base_scraper.py 重构 zhihu_scraper_v2.py:

1. 继承 BaseSocialScraper
2. 使用显式等待替代 time.sleep()
3. 添加 @retry_on_failure 装饰器
4. 多选择器策略(应对前端变化)
5. 结构化日志(替代print)

创建文件: scrapers/zhihu_scraper_pro.py

#### 任务2: 创建统一管理器
创建 scrapers/scraper_manager_pro.py:

功能:
- 统一调度小红书/知乎爬虫
- 并发控制(避免被封)
- 错误恢复(失败自动重试)
- 进度追踪(实时显示)
- 数据去重和合并

#### 任务3: 编写测试
创建 tests/test_scraper_pro.py:

测试用例:
- 测试基类初始化
- 测试显式等待
- 测试重试机制
- 测试智能滚动
- 测试错误处理

### 输出:
1. scrapers/zhihu_scraper_pro.py (完整代码)
2. scrapers/scraper_manager_pro.py (完整代码)
3. tests/test_scraper_pro.py (测试代码)
4. REFACTOR_SUMMARY.md (重构总结,包括:改进点、性能对比、使用示例)

开始执行!
```

---

## 每日工作流模板

### 早上启动
```
早上好!今天的任务:

1. 检查昨天的进度
   - 完成了什么?
   - 遇到了什么问题?
   - 数据质量如何?

2. 分析系统状态
   - 爬虫是否正常?
   - 邮件发送情况?
   - 有没有错误日志?

3. 生成今日任务清单
   - TOP 3优先任务
   - 预计完成时间
   - 需要的资源

开始!
```

### 遇到问题
```
我遇到了以下问题:

**问题描述:**
[详细描述问题]

**环境信息:**
- 操作系统: Windows 11
- Python版本: 3.9
- 相关依赖: [列出]

**错误日志:**
```
[粘贴完整错误日志]
```

**相关代码:**
```python
[粘贴相关代码]
```

**我尝试过:**
1. [尝试1]
2. [尝试2]

请帮我:
1. 分析问题根本原因
2. 提供解决方案(至少2个)
3. 给出完整的修复代码
4. 解释为什么会出现这个问题
5. 如何避免类似问题

谢谢!
```

### 晚上总结
```
今天的工作总结:

**完成的任务:**
- [任务1] ✅
- [任务2] ✅
- [任务3] ⏳ (进行中)

**遇到的问题:**
- [问题1] - 已解决
- [问题2] - 待解决

**数据:**
- 新增客户: X个
- 发送邮件: X封
- 打开率: X%
- 回复: X人

请帮我:
1. 评估今天的进度(是否达到预期?)
2. 分析遇到的问题(根本原因是什么?)
3. 制定明天的计划(TOP 3任务)
4. 给出改进建议

晚安!
```

---

## 🎯 立即开始

**复制上面的"第一次对话"Prompt,粘贴到Gemini,开始你的AI合伙人之旅!**

**GitHub项目:** https://github.com/emptyteabot/HUOKE

**预计30天后,你将拥有:**
- ✅ 150-200个真实客户数据
- ✅ 生产级代码
- ✅ 自动化系统
- ✅ 第一批付费客户
- ✅ ¥12-15万营收

**Gemini会帮你完成90%的工作,你只需要做决策和审核!**
