# é‡è¯•è£…é¥°å™¨å¿«é€Ÿå‚è€ƒ

## å¯¼å…¥

```python
from retry_decorator import scraper_retry, critical_operation_retry, quick_retry
```

---

## ä¸‰ç§ç­–ç•¥

### 1. æ ‡å‡†é‡è¯• `@scraper_retry`

**ç”¨é€”**: ä¸€èˆ¬çˆ¬å–æ“ä½œ

```python
@scraper_retry(max_attempts=3, min_wait=2, max_wait=8)
def scrape_comments(self, url):
    # çˆ¬å–è¯„è®º
    pass
```

- æœ€å¤§é‡è¯•: 3æ¬¡
- ç­‰å¾…æ—¶é—´: 2-8ç§’
- æŒ‡æ•°é€€é¿: 2ç§’ â†’ 4ç§’ â†’ 8ç§’

---

### 2. å…³é”®æ“ä½œé‡è¯• `@critical_operation_retry`

**ç”¨é€”**: åˆå§‹åŒ–ã€ç™»å½•ç­‰å…³é”®æ“ä½œ

```python
@critical_operation_retry(max_attempts=5)
def init_driver(self):
    # åˆå§‹åŒ–æµè§ˆå™¨
    pass
```

- æœ€å¤§é‡è¯•: 5æ¬¡
- ç­‰å¾…æ—¶é—´: 3-15ç§’
- æŒ‡æ•°é€€é¿: 3ç§’ â†’ 6ç§’ â†’ 12ç§’

---

### 3. å¿«é€Ÿé‡è¯• `@quick_retry`

**ç”¨é€”**: ç®€å•æå–ã€å¿«é€Ÿæ“ä½œ

```python
@quick_retry(max_attempts=2)
def extract_info(self, element):
    # æå–ä¿¡æ¯
    pass
```

- æœ€å¤§é‡è¯•: 2æ¬¡
- ç­‰å¾…æ—¶é—´: 1-3ç§’
- æŒ‡æ•°é€€é¿: 1ç§’ â†’ 2ç§’

---

## å¯é‡è¯•é”™è¯¯

âœ“ `TimeoutException` - è¶…æ—¶
âœ“ `NoSuchElementException` - å…ƒç´ æœªæ‰¾åˆ°
âœ“ `StaleElementReferenceException` - å…ƒç´ è¿‡æœŸ
âœ“ `ConnectionError` - è¿æ¥é”™è¯¯

---

## ä¸å¯é‡è¯•é”™è¯¯

âœ— `KeyboardInterrupt` - ç”¨æˆ·ä¸­æ–­
âœ— `SystemExit` - ç³»ç»Ÿé€€å‡º
âœ— `MemoryError` - å†…å­˜é”™è¯¯

---

## è‡ªå®šä¹‰å‚æ•°

```python
@scraper_retry(
    max_attempts=5,      # æœ€å¤šé‡è¯•5æ¬¡
    min_wait=3,          # æœ€å°ç­‰å¾…3ç§’
    max_wait=15,         # æœ€å¤§ç­‰å¾…15ç§’
    multiplier=2         # æŒ‡æ•°ä¹˜æ•°
)
def custom_scrape(self):
    pass
```

---

## æ‰‹åŠ¨é‡è¯•

```python
from retry_decorator import retry_on_failure

success, result, error = retry_on_failure(
    func=scrape_function,
    max_attempts=3,
    delay=2,
    url="https://example.com"
)
```

---

## å®Œæ•´ç¤ºä¾‹

```python
from retry_decorator import scraper_retry, critical_operation_retry, quick_retry

class MyScraper:
    @critical_operation_retry(max_attempts=3)
    def init_driver(self):
        """åˆå§‹åŒ– - å…³é”®æ“ä½œ"""
        self.driver = uc.Chrome()
        return True

    @scraper_retry(max_attempts=3, min_wait=2, max_wait=8)
    def search(self, keyword):
        """æœç´¢ - æ ‡å‡†é‡è¯•"""
        self.driver.get(f"https://example.com/search?q={keyword}")
        return True

    @scraper_retry(max_attempts=3, min_wait=2, max_wait=8)
    def scrape_page(self, url):
        """çˆ¬å–é¡µé¢ - æ ‡å‡†é‡è¯•"""
        self.driver.get(url)
        data = self._extract_data()
        return data

    @quick_retry(max_attempts=2)
    def _extract_data(self):
        """æå–æ•°æ® - å¿«é€Ÿé‡è¯•"""
        element = self.driver.find_element(By.CSS_SELECTOR, ".data")
        return element.text
```

---

## æµ‹è¯•

```bash
# éªŒè¯å®‰è£…
python verify_retry.py

# è¿è¡Œæµ‹è¯•
python test_retry.py
```

---

## æ–‡æ¡£

- `ERROR_HANDLING_GUIDE.md` - è¯¦ç»†æŒ‡å—
- `RETRY_UPGRADE_README.md` - å‡çº§è¯´æ˜
- `COMPLETION_SUMMARY.md` - å®Œæˆæ€»ç»“

---

## é€‰æ‹©ç­–ç•¥

| æ“ä½œç±»å‹ | ä½¿ç”¨ç­–ç•¥ | é‡è¯•æ¬¡æ•° |
|---------|---------|---------|
| æµè§ˆå™¨åˆå§‹åŒ– | `@critical_operation_retry` | 5æ¬¡ |
| ç™»å½• | `@critical_operation_retry` | 5æ¬¡ |
| æœç´¢ | `@scraper_retry` | 3æ¬¡ |
| è·å–é“¾æ¥ | `@scraper_retry` | 3æ¬¡ |
| çˆ¬å–è¯„è®º | `@scraper_retry` | 3æ¬¡ |
| æå–ä¿¡æ¯ | `@quick_retry` | 2æ¬¡ |
| è§£ææ•°æ® | `@quick_retry` | 2æ¬¡ |

---

## æ—¥å¿—ç¤ºä¾‹

```
2024-02-22 10:30:15 - INFO - å°è¯•æ‰§è¡Œ scrape_comments (ç¬¬ 1/3 æ¬¡)
2024-02-22 10:30:20 - WARNING - ç¬¬ 1 æ¬¡å°è¯•å¤±è´¥: TimeoutException
2024-02-22 10:30:20 - INFO - ç­‰å¾… 2 ç§’åé‡è¯•...
2024-02-22 10:30:25 - INFO - âœ“ scrape_comments æ‰§è¡ŒæˆåŠŸ
```

---

**å¿«é€Ÿå‚è€ƒå®Œæˆ!** ğŸš€
