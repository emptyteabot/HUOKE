"""
LinkedInè‡ªåŠ¨æŠ“å–æœåŠ¡ - åŸºäºlinkedin_scraper (3.7kâ­)
ä½¿ç”¨Playwrightå®ç°å¼‚æ­¥æŠ“å–
"""

from playwright.async_api import async_playwright, Browser, Page
import json
import asyncio
from typing import List, Dict, Optional
from datetime import datetime

class LinkedInAutoScraper:
    def __init__(self):
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.session_file = "linkedin_session.json"

    async def init_browser(self, headless: bool = True):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(
            headless=headless,
            args=[
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage'
            ]
        )

        # åˆ›å»ºä¸Šä¸‹æ–‡
        context = await self.browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        )

        self.page = await context.new_page()

        # å°è¯•åŠ è½½å·²ä¿å­˜çš„ä¼šè¯
        await self.load_session()

    async def load_session(self):
        """åŠ è½½å·²ä¿å­˜çš„ä¼šè¯"""
        try:
            with open(self.session_file, 'r') as f:
                cookies = json.load(f)
                await self.page.context.add_cookies(cookies)
                print("âœ… å·²åŠ è½½ä¿å­˜çš„ä¼šè¯")
        except FileNotFoundError:
            print("âš ï¸ æœªæ‰¾åˆ°ä¿å­˜çš„ä¼šè¯,éœ€è¦ç™»å½•")

    async def save_session(self):
        """ä¿å­˜ä¼šè¯"""
        cookies = await self.page.context.cookies()
        with open(self.session_file, 'w') as f:
            json.dump(cookies, f)
        print("âœ… ä¼šè¯å·²ä¿å­˜")

    async def login(self, email: str, password: str):
        """è‡ªåŠ¨ç™»å½•LinkedIn"""
        print("ğŸ” å¼€å§‹ç™»å½•LinkedIn...")

        await self.page.goto('https://www.linkedin.com/login')
        await self.page.wait_for_load_state('networkidle')

        # è¾“å…¥å‡­è¯
        await self.page.fill('#username', email)
        await self.page.fill('#password', password)

        # ç‚¹å‡»ç™»å½•
        await self.page.click('button[type="submit"]')

        # ç­‰å¾…ç™»å½•å®Œæˆ
        try:
            await self.page.wait_for_url('**/feed/**', timeout=30000)
            print("âœ… ç™»å½•æˆåŠŸ!")
            await self.save_session()
            return True
        except:
            print("âŒ ç™»å½•å¤±è´¥,å¯èƒ½éœ€è¦éªŒè¯")
            return False

    async def search_education_leads(
        self,
        keywords: str = "ç•™å­¦ OR study abroad OR å‡ºå›½",
        location: str = "ä¸­å›½",
        limit: int = 50
    ) -> List[Dict]:
        """æœç´¢ç•™å­¦ç›¸å…³æ½œåœ¨å®¢æˆ·"""
        print(f"ğŸ” æœç´¢å…³é”®è¯: {keywords}, åœ°åŒº: {location}")

        # æ„å»ºæœç´¢URL
        search_url = f"https://www.linkedin.com/search/results/people/?keywords={keywords}&geoUrn={location}"
        await self.page.goto(search_url)
        await self.page.wait_for_load_state('networkidle')

        leads = []

        # æ»šåŠ¨åŠ è½½æ›´å¤šç»“æœ
        for i in range(limit // 10):
            await self.page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            await asyncio.sleep(2)
            print(f"ğŸ“œ å·²æ»šåŠ¨ {i+1} æ¬¡...")

        # æå–æ•°æ®
        results = await self.page.query_selector_all('.reusable-search__result-container')

        for result in results[:limit]:
            try:
                # æå–å§“å
                name_elem = await result.query_selector('.entity-result__title-text a')
                name = await name_elem.inner_text() if name_elem else ""

                # æå–æ ‡é¢˜/èŒä½
                title_elem = await result.query_selector('.entity-result__primary-subtitle')
                title = await title_elem.inner_text() if title_elem else ""

                # æå–å…¬å¸/å­¦æ ¡
                company_elem = await result.query_selector('.entity-result__secondary-subtitle')
                company = await company_elem.inner_text() if company_elem else ""

                # æå–åœ°åŒº
                location_elem = await result.query_selector('.entity-result__location')
                location = await location_elem.inner_text() if location_elem else ""

                # æå–LinkedIn URL
                profile_url = await name_elem.get_attribute('href') if name_elem else ""

                # è¯„åˆ†é€»è¾‘
                score = self.calculate_lead_score(title, company, location)

                lead = {
                    'name': name.strip(),
                    'title': title.strip(),
                    'company': company.strip(),
                    'location': location.strip(),
                    'linkedin_url': profile_url,
                    'score': score,
                    'source': 'linkedin',
                    'scraped_at': datetime.now().isoformat()
                }

                leads.append(lead)
                print(f"âœ… æå–: {name} - {title} (è¯„åˆ†: {score})")

            except Exception as e:
                print(f"âš ï¸ æå–å¤±è´¥: {e}")
                continue

        print(f"ğŸ‰ å…±æå– {len(leads)} ä¸ªæ½œåœ¨å®¢æˆ·")
        return leads

    def calculate_lead_score(self, title: str, company: str, location: str) -> int:
        """è®¡ç®—çº¿ç´¢è¯„åˆ†"""
        score = 0

        # æ ‡é¢˜è¯„åˆ†
        high_value_keywords = ['å­¦ç”Ÿ', 'å®¶é•¿', 'æ•™è‚²', 'ç•™å­¦é¡¾é—®', 'å‡å­¦']
        for keyword in high_value_keywords:
            if keyword in title:
                score += 15

        # å…¬å¸è¯„åˆ†
        if any(word in company for word in ['å¤§å­¦', 'å­¦é™¢', 'é«˜ä¸­', 'ä¸­å­¦']):
            score += 10

        # åœ°åŒºè¯„åˆ†
        tier1_cities = ['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³', 'å¹¿å·', 'æ­å·']
        if any(city in location for city in tier1_cities):
            score += 5

        return score

    async def get_profile_details(self, profile_url: str) -> Dict:
        """è·å–ä¸ªäººèµ„æ–™è¯¦æƒ…"""
        print(f"ğŸ“„ è·å–èµ„æ–™: {profile_url}")

        await self.page.goto(profile_url)
        await self.page.wait_for_load_state('networkidle')

        # æå–è¯¦ç»†ä¿¡æ¯
        details = {}

        try:
            # å…³äº
            about_elem = await self.page.query_selector('#about + div .inline-show-more-text')
            details['about'] = await about_elem.inner_text() if about_elem else ""

            # ç»å†
            experiences = []
            exp_items = await self.page.query_selector_all('#experience + div li')
            for item in exp_items[:3]:  # åªå–å‰3ä¸ª
                exp_text = await item.inner_text()
                experiences.append(exp_text)
            details['experiences'] = experiences

            # æ•™è‚²èƒŒæ™¯
            educations = []
            edu_items = await self.page.query_selector_all('#education + div li')
            for item in edu_items[:3]:
                edu_text = await item.inner_text()
                educations.append(edu_text)
            details['educations'] = educations

        except Exception as e:
            print(f"âš ï¸ æå–è¯¦æƒ…å¤±è´¥: {e}")

        return details

    async def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.browser:
            await self.browser.close()
            print("ğŸ”’ æµè§ˆå™¨å·²å…³é—­")


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    scraper = LinkedInAutoScraper()

    try:
        # åˆå§‹åŒ–
        await scraper.init_browser(headless=False)

        # ç™»å½•
        await scraper.login(
            email="your@email.com",
            password="your_password"
        )

        # æœç´¢ç•™å­¦ç›¸å…³æ½œåœ¨å®¢æˆ·
        leads = await scraper.search_education_leads(
            keywords="ç•™å­¦ OR study abroad OR æ‰˜ç¦ OR é›…æ€",
            location="ä¸­å›½",
            limit=50
        )

        # ä¿å­˜åˆ°JSON
        with open('leads.json', 'w', encoding='utf-8') as f:
            json.dump(leads, f, ensure_ascii=False, indent=2)

        print(f"âœ… å·²ä¿å­˜ {len(leads)} ä¸ªæ½œåœ¨å®¢æˆ·åˆ° leads.json")

    finally:
        await scraper.close()


if __name__ == '__main__':
    asyncio.run(main())
