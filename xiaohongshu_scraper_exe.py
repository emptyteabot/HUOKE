"""
å°çº¢ä¹¦è‡ªåŠ¨è·å®¢å·¥å…· - ç‹¬ç«‹exeç‰ˆæœ¬

åŠŸèƒ½:
1. ç™»å½•ä½ çš„å°çº¢ä¹¦è´¦å·
2. æœç´¢å…³é”®è¯(ä¾‹å¦‚: "ç¾å›½ç•™å­¦")
3. è‡ªåŠ¨çˆ¬å–ç¬”è®°è¯„è®ºåŒº
4. æå–çœŸå®ç”¨æˆ·ä¿¡æ¯
5. ä¿å­˜åˆ°Excel

ä½¿ç”¨æ–¹æ³•:
python xiaohongshu_scraper_exe.py
"""

import time
import random
import json
import pandas as pd
from datetime import datetime
from typing import List, Dict
import os

try:
    import undetected_chromedriver as uc
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.common.keys import Keys
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("âŒ è¯·å…ˆå®‰è£…ä¾èµ–: pip install selenium undetected-chromedriver pandas openpyxl")
    exit(1)


class XiaohongshuAutoScraper:
    """å°çº¢ä¹¦è‡ªåŠ¨è·å®¢å·¥å…·"""

    def __init__(self, headless: bool = False):
        self.headless = headless
        self.driver = None
        self.logged_in = False
        self.results = []

    def init_driver(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        print("ğŸš€ å¯åŠ¨æµè§ˆå™¨...")

        options = uc.ChromeOptions()
        if self.headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')

        # éšæœºUser-Agent
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]
        options.add_argument(f'user-agent={random.choice(user_agents)}')

        self.driver = uc.Chrome(options=options)
        self.driver.set_page_load_timeout(30)
        self.driver.maximize_window()

        print("âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")

    def login(self):
        """ç™»å½•å°çº¢ä¹¦"""
        print("\n" + "="*50)
        print("ğŸ” ç™»å½•å°çº¢ä¹¦")
        print("="*50)

        self.driver.get("https://www.xiaohongshu.com")
        time.sleep(3)

        print("\nè¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•å°çº¢ä¹¦:")
        print("1. æ‰«ç ç™»å½• æˆ– æ‰‹æœºå·ç™»å½•")
        print("2. ç™»å½•æˆåŠŸå,æŒ‰å›è½¦ç»§ç»­...")

        input("\næŒ‰å›è½¦é”®ç»§ç»­...")

        # æ£€æŸ¥æ˜¯å¦ç™»å½•æˆåŠŸ
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·å¤´åƒ
            self.driver.find_element(By.CSS_SELECTOR, ".avatar, .user-avatar")
            print("âœ… ç™»å½•æˆåŠŸ!")
            self.logged_in = True
            return True
        except:
            print("âš ï¸ æœªæ£€æµ‹åˆ°ç™»å½•,ç»§ç»­å°è¯•...")
            self.logged_in = True  # å‡è®¾ç™»å½•æˆåŠŸ
            return True

    def random_sleep(self, min_sec: float = 2, max_sec: float = 5):
        """éšæœºå»¶è¿Ÿ"""
        sleep_time = random.uniform(min_sec, max_sec)
        time.sleep(sleep_time)

    def search_keyword(self, keyword: str):
        """æœç´¢å…³é”®è¯"""
        print(f"\nğŸ” æœç´¢å…³é”®è¯: {keyword}")

        try:
            # è®¿é—®æœç´¢é¡µé¢
            search_url = f"https://www.xiaohongshu.com/search_result?keyword={keyword}&source=web_search_result_notes"
            self.driver.get(search_url)
            self.random_sleep(3, 5)

            print("âœ… æœç´¢é¡µé¢åŠ è½½æˆåŠŸ")
            return True

        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return False

    def scroll_and_load(self, scroll_times: int = 3):
        """æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹"""
        print(f"ğŸ“œ æ»šåŠ¨åŠ è½½æ›´å¤šç¬”è®°...")

        for i in range(scroll_times):
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            print(f"  æ»šåŠ¨ {i+1}/{scroll_times}")
            self.random_sleep(2, 3)

    def get_note_links(self, limit: int = 20) -> List[str]:
        """è·å–ç¬”è®°é“¾æ¥"""
        print(f"\nğŸ“ è·å–ç¬”è®°é“¾æ¥ (ç›®æ ‡: {limit}ä¸ª)")

        note_links = []

        try:
            # æŸ¥æ‰¾ç¬”è®°å¡ç‰‡
            # å°çº¢ä¹¦çš„DOMç»“æ„å¯èƒ½æ˜¯: a.cover, a[href*="/explore/"]
            note_elements = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/explore/']")

            print(f"  æ‰¾åˆ° {len(note_elements)} ä¸ªç¬”è®°")

            for element in note_elements[:limit]:
                try:
                    link = element.get_attribute("href")
                    if link and link not in note_links:
                        note_links.append(link)
                except:
                    continue

            print(f"âœ… æˆåŠŸè·å– {len(note_links)} ä¸ªç¬”è®°é“¾æ¥")

        except Exception as e:
            print(f"âŒ è·å–ç¬”è®°é“¾æ¥å¤±è´¥: {e}")

        return note_links

    def scrape_note_comments(self, note_url: str) -> List[Dict]:
        """çˆ¬å–å•ä¸ªç¬”è®°çš„è¯„è®º"""
        print(f"\nğŸ“– çˆ¬å–ç¬”è®°: {note_url}")

        comments = []

        try:
            self.driver.get(note_url)
            self.random_sleep(3, 5)

            # æ»šåŠ¨åˆ°è¯„è®ºåŒº
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            self.random_sleep(2, 3)

            # æŸ¥æ‰¾è¯„è®ºå…ƒç´ 
            # å°çº¢ä¹¦è¯„è®ºçš„DOMç»“æ„å¯èƒ½æ˜¯: .comment-item, .note-comment-item
            comment_elements = self.driver.find_elements(By.CSS_SELECTOR, ".comment-item, .note-comment-item, [class*='comment']")

            print(f"  æ‰¾åˆ° {len(comment_elements)} æ¡è¯„è®º")

            for idx, comment_elem in enumerate(comment_elements[:50]):  # æœ€å¤š50æ¡
                try:
                    # æå–ç”¨æˆ·å
                    try:
                        username_elem = comment_elem.find_element(By.CSS_SELECTOR, ".username, .user-name, [class*='username']")
                        username = username_elem.text.strip()
                    except:
                        username = "æœªçŸ¥ç”¨æˆ·"

                    # æå–è¯„è®ºå†…å®¹
                    try:
                        content_elem = comment_elem.find_element(By.CSS_SELECTOR, ".content, .comment-content, [class*='content']")
                        content = content_elem.text.strip()
                    except:
                        content = ""

                    # æå–ç”¨æˆ·ä¸»é¡µé“¾æ¥
                    try:
                        user_link_elem = comment_elem.find_element(By.CSS_SELECTOR, "a[href*='/user/']")
                        user_link = user_link_elem.get_attribute("href")
                    except:
                        user_link = ""

                    if username and content:
                        comment_data = {
                            'username': username,
                            'content': content,
                            'user_link': user_link,
                            'note_url': note_url,
                            'scraped_at': datetime.now().isoformat()
                        }

                        comments.append(comment_data)
                        print(f"    âœ… [{idx+1}] {username}: {content[:30]}...")

                except Exception as e:
                    print(f"    âš ï¸ è§£æè¯„è®ºå¤±è´¥: {e}")
                    continue

            print(f"âœ… æˆåŠŸçˆ¬å– {len(comments)} æ¡è¯„è®º")

        except Exception as e:
            print(f"âŒ çˆ¬å–è¯„è®ºå¤±è´¥: {e}")

        return comments

    def filter_high_intent_comments(self, comments: List[Dict]) -> List[Dict]:
        """ç­›é€‰é«˜æ„å‘è¯„è®º"""
        print(f"\nğŸ¯ ç­›é€‰é«˜æ„å‘è¯„è®º...")

        # é«˜æ„å‘å…³é”®è¯
        intent_keywords = [
            "æƒ³å’¨è¯¢", "æ±‚æ¨è", "æ€ä¹ˆç”³è¯·", "æœ‰æ²¡æœ‰", "æ±‚è”ç³»",
            "åŠ å¾®ä¿¡", "ç§ä¿¡", "æ±‚åŠ©", "è¯·é—®", "äº†è§£ä¸€ä¸‹",
            "æƒ³å»", "æ‰“ç®—", "å‡†å¤‡", "è€ƒè™‘", "æœ‰æ„å‘"
        ]

        high_intent_comments = []

        for comment in comments:
            content = comment['content'].lower()

            # æ£€æŸ¥æ˜¯å¦åŒ…å«é«˜æ„å‘å…³é”®è¯
            if any(keyword in content for keyword in intent_keywords):
                comment['intent_level'] = 'high'
                high_intent_comments.append(comment)
            else:
                comment['intent_level'] = 'low'

        print(f"âœ… æ‰¾åˆ° {len(high_intent_comments)} æ¡é«˜æ„å‘è¯„è®º")

        return high_intent_comments

    def save_to_excel(self, comments: List[Dict], filename: str = None):
        """ä¿å­˜åˆ°Excel"""
        if not filename:
            filename = f"xiaohongshu_leads_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"

        print(f"\nğŸ’¾ ä¿å­˜åˆ°Excel: {filename}")

        try:
            df = pd.DataFrame(comments)

            # é‡æ–°æ’åˆ—åˆ—é¡ºåº
            columns = ['username', 'content', 'intent_level', 'user_link', 'note_url', 'scraped_at']
            df = df[columns]

            # ä¿å­˜åˆ°Excel
            df.to_excel(filename, index=False, engine='openpyxl')

            print(f"âœ… æˆåŠŸä¿å­˜ {len(comments)} æ¡æ•°æ®åˆ° {filename}")
            print(f"ğŸ“‚ æ–‡ä»¶ä½ç½®: {os.path.abspath(filename)}")

        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")

    def run(self, keyword: str, note_limit: int = 10):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        print("\n" + "="*50)
        print("ğŸš€ å°çº¢ä¹¦è‡ªåŠ¨è·å®¢å·¥å…·")
        print("="*50)

        try:
            # 1. åˆå§‹åŒ–æµè§ˆå™¨
            self.init_driver()

            # 2. ç™»å½•
            if not self.login():
                print("âŒ ç™»å½•å¤±è´¥,é€€å‡º")
                return

            # 3. æœç´¢å…³é”®è¯
            if not self.search_keyword(keyword):
                print("âŒ æœç´¢å¤±è´¥,é€€å‡º")
                return

            # 4. æ»šåŠ¨åŠ è½½
            self.scroll_and_load(3)

            # 5. è·å–ç¬”è®°é“¾æ¥
            note_links = self.get_note_links(note_limit)

            if not note_links:
                print("âŒ æœªæ‰¾åˆ°ç¬”è®°,é€€å‡º")
                return

            # 6. çˆ¬å–æ¯ä¸ªç¬”è®°çš„è¯„è®º
            all_comments = []

            for idx, note_url in enumerate(note_links):
                print(f"\nè¿›åº¦: {idx+1}/{len(note_links)}")

                comments = self.scrape_note_comments(note_url)
                all_comments.extend(comments)

                # éšæœºå»¶è¿Ÿ,é¿å…è¢«å°
                self.random_sleep(3, 6)

            print(f"\n" + "="*50)
            print(f"âœ… æ€»å…±çˆ¬å– {len(all_comments)} æ¡è¯„è®º")
            print("="*50)

            # 7. ç­›é€‰é«˜æ„å‘è¯„è®º
            high_intent_comments = self.filter_high_intent_comments(all_comments)

            # 8. ä¿å­˜åˆ°Excel
            self.save_to_excel(all_comments)

            # 9. å•ç‹¬ä¿å­˜é«˜æ„å‘è¯„è®º
            if high_intent_comments:
                high_intent_filename = f"xiaohongshu_high_intent_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
                self.save_to_excel(high_intent_comments, high_intent_filename)

            print("\n" + "="*50)
            print("ğŸ‰ å®Œæˆ!")
            print("="*50)
            print(f"æ€»è¯„è®ºæ•°: {len(all_comments)}")
            print(f"é«˜æ„å‘è¯„è®º: {len(high_intent_comments)}")

        except Exception as e:
            print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")

        finally:
            # å…³é—­æµè§ˆå™¨
            if self.driver:
                print("\nâ¸ï¸ æŒ‰å›è½¦å…³é—­æµè§ˆå™¨...")
                input()
                self.driver.quit()


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*50)
    print("ğŸ¯ å°çº¢ä¹¦è‡ªåŠ¨è·å®¢å·¥å…·")
    print("="*50)

    # è¾“å…¥å‚æ•°
    keyword = input("\nè¯·è¾“å…¥æœç´¢å…³é”®è¯ (ä¾‹å¦‚: ç¾å›½ç•™å­¦): ").strip()
    if not keyword:
        keyword = "ç¾å›½ç•™å­¦"

    note_limit = input("è¯·è¾“å…¥è¦çˆ¬å–çš„ç¬”è®°æ•°é‡ (é»˜è®¤10): ").strip()
    if not note_limit:
        note_limit = 10
    else:
        note_limit = int(note_limit)

    headless = input("æ˜¯å¦åå°è¿è¡Œ? (y/n, é»˜è®¤n): ").strip().lower() == 'y'

    # è¿è¡Œ
    scraper = XiaohongshuAutoScraper(headless=headless)
    scraper.run(keyword, note_limit)


if __name__ == "__main__":
    main()
