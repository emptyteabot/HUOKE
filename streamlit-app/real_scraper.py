"""
çœŸå®çš„å¤šå¹³å°æ•°æ®æŠ“å–æ¨¡å—

ä½¿ç”¨Selenium + åæ£€æµ‹æŠ€æœ¯å®ç°çœŸå®çš„æ•°æ®æŠ“å–
æ”¯æŒ: LinkedIn, å°çº¢ä¹¦, çŸ¥ä¹

æ³¨æ„: éœ€è¦å®‰è£… selenium, undetected-chromedriver
pip install selenium undetected-chromedriver
"""

import time
import random
import json
from typing import Dict, List, Optional
from datetime import datetime
import re

try:
    import undetected_chromedriver as uc
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("âš ï¸ Seleniumæœªå®‰è£…,è¯·è¿è¡Œ: pip install selenium undetected-chromedriver")


class BaseScraper:
    """åŸºç¡€çˆ¬è™«ç±»"""

    def __init__(self, headless: bool = True):
        self.headless = headless
        self.driver = None

    def init_driver(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        if not SELENIUM_AVAILABLE:
            raise ImportError("è¯·å…ˆå®‰è£…: pip install selenium undetected-chromedriver")

        options = uc.ChromeOptions()
        if self.headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')

        # éšæœºUser-Agent
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]
        options.add_argument(f'user-agent={random.choice(user_agents)}')

        self.driver = uc.Chrome(options=options)
        self.driver.set_page_load_timeout(30)

    def random_sleep(self, min_sec: float = 1, max_sec: float = 3):
        """éšæœºå»¶è¿Ÿ,æ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
        time.sleep(random.uniform(min_sec, max_sec))

    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            self.driver.quit()

    def __enter__(self):
        self.init_driver()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()


class XiaohongshuScraper(BaseScraper):
    """å°çº¢ä¹¦çœŸå®æŠ“å–"""

    def __init__(self, headless: bool = True):
        super().__init__(headless)
        self.base_url = "https://www.xiaohongshu.com"

    def search_notes(self, keywords: str, limit: int = 20) -> List[Dict]:
        """
        æœç´¢å°çº¢ä¹¦ç¬”è®°

        Args:
            keywords: æœç´¢å…³é”®è¯(ä¾‹å¦‚: "ç¾å›½ç•™å­¦")
            limit: è¿”å›æ•°é‡

        Returns:
            List[Dict]: ç¬”è®°åˆ—è¡¨
        """
        if not self.driver:
            self.init_driver()

        results = []

        try:
            # è®¿é—®æœç´¢é¡µé¢
            search_url = f"{self.base_url}/search_result?keyword={keywords}&source=web_search_result_notes"
            print(f"ğŸ” æ­£åœ¨æœç´¢å°çº¢ä¹¦: {keywords}")
            self.driver.get(search_url)
            self.random_sleep(3, 5)

            # æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹
            for _ in range(3):
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                self.random_sleep(2, 3)

            # æŸ¥æ‰¾ç¬”è®°å¡ç‰‡
            # å°çº¢ä¹¦çš„DOMç»“æ„: section.note-item
            note_elements = self.driver.find_elements(By.CSS_SELECTOR, "section.note-item, div.note-item")

            print(f"ğŸ“ æ‰¾åˆ° {len(note_elements)} æ¡ç¬”è®°")

            for idx, note in enumerate(note_elements[:limit]):
                try:
                    # æå–æ ‡é¢˜
                    title_elem = note.find_element(By.CSS_SELECTOR, ".title, .note-title")
                    title = title_elem.text.strip()

                    # æå–ä½œè€…
                    try:
                        author_elem = note.find_element(By.CSS_SELECTOR, ".author, .username")
                        author = author_elem.text.strip()
                    except:
                        author = "æœªçŸ¥ç”¨æˆ·"

                    # æå–é“¾æ¥
                    try:
                        link_elem = note.find_element(By.CSS_SELECTOR, "a")
                        note_url = link_elem.get_attribute("href")
                        if not note_url.startswith("http"):
                            note_url = self.base_url + note_url
                    except:
                        note_url = ""

                    # æå–äº’åŠ¨æ•°æ®
                    try:
                        likes_elem = note.find_element(By.CSS_SELECTOR, ".like-count, .likes")
                        likes = int(re.sub(r'\D', '', likes_elem.text))
                    except:
                        likes = 0

                    results.append({
                        'title': title,
                        'author': author,
                        'url': note_url,
                        'likes': likes,
                        'source': 'xiaohongshu',
                        'keywords': keywords,
                        'scraped_at': datetime.now().isoformat()
                    })

                    print(f"  âœ… [{idx+1}] {title[:30]}... - {author}")

                except Exception as e:
                    print(f"  âš ï¸ è§£æç¬”è®°å¤±è´¥: {e}")
                    continue

            print(f"âœ… æˆåŠŸæŠ“å– {len(results)} æ¡å°çº¢ä¹¦ç¬”è®°")

        except Exception as e:
            print(f"âŒ å°çº¢ä¹¦æŠ“å–å¤±è´¥: {e}")

        return results

    def get_note_detail(self, note_url: str) -> Dict:
        """è·å–ç¬”è®°è¯¦æƒ…"""
        if not self.driver:
            self.init_driver()

        try:
            self.driver.get(note_url)
            self.random_sleep(2, 4)

            # æå–è¯¦ç»†å†…å®¹
            content_elem = self.driver.find_element(By.CSS_SELECTOR, ".content, .note-content")
            content = content_elem.text.strip()

            # æå–è¯„è®º
            comments = []
            comment_elements = self.driver.find_elements(By.CSS_SELECTOR, ".comment-item")

            for comment in comment_elements[:10]:  # åªå–å‰10æ¡è¯„è®º
                try:
                    author = comment.find_element(By.CSS_SELECTOR, ".username").text
                    text = comment.find_element(By.CSS_SELECTOR, ".comment-text").text
                    comments.append({'author': author, 'text': text})
                except:
                    continue

            return {
                'content': content,
                'comments': comments,
                'scraped_at': datetime.now().isoformat()
            }

        except Exception as e:
            print(f"âŒ è·å–ç¬”è®°è¯¦æƒ…å¤±è´¥: {e}")
            return {}


class LinkedInScraper(BaseScraper):
    """LinkedInçœŸå®æŠ“å– (éœ€è¦ç™»å½•)"""

    def __init__(self, email: str = "", password: str = "", headless: bool = True):
        super().__init__(headless)
        self.email = email
        self.password = password
        self.base_url = "https://www.linkedin.com"
        self.logged_in = False

    def login(self):
        """ç™»å½•LinkedIn"""
        if not self.driver:
            self.init_driver()

        if not self.email or not self.password:
            print("âš ï¸ æœªæä¾›LinkedInè´¦å·,å°†ä½¿ç”¨å…¬å¼€æœç´¢(æ•°æ®æœ‰é™)")
            return False

        try:
            print("ğŸ” æ­£åœ¨ç™»å½•LinkedIn...")
            self.driver.get(f"{self.base_url}/login")
            self.random_sleep(2, 3)

            # è¾“å…¥é‚®ç®±
            email_input = self.driver.find_element(By.ID, "username")
            email_input.send_keys(self.email)
            self.random_sleep(0.5, 1)

            # è¾“å…¥å¯†ç 
            password_input = self.driver.find_element(By.ID, "password")
            password_input.send_keys(self.password)
            self.random_sleep(0.5, 1)

            # ç‚¹å‡»ç™»å½•
            login_btn = self.driver.find_element(By.CSS_SELECTOR, "button[type='submit']")
            login_btn.click()
            self.random_sleep(3, 5)

            # æ£€æŸ¥æ˜¯å¦ç™»å½•æˆåŠŸ
            if "feed" in self.driver.current_url or "mynetwork" in self.driver.current_url:
                print("âœ… LinkedInç™»å½•æˆåŠŸ")
                self.logged_in = True
                return True
            else:
                print("âŒ LinkedInç™»å½•å¤±è´¥")
                return False

        except Exception as e:
            print(f"âŒ LinkedInç™»å½•å¤±è´¥: {e}")
            return False

    def search_people(self, keywords: str, location: str = "", limit: int = 20) -> List[Dict]:
        """
        æœç´¢LinkedInç”¨æˆ·

        Args:
            keywords: æœç´¢å…³é”®è¯(ä¾‹å¦‚: "study abroad consultant")
            location: åœ°åŒº
            limit: è¿”å›æ•°é‡

        Returns:
            List[Dict]: ç”¨æˆ·åˆ—è¡¨
        """
        if not self.driver:
            self.init_driver()

        # å¦‚æœæœªç™»å½•,å°è¯•ç™»å½•
        if not self.logged_in:
            self.login()

        results = []

        try:
            # æ„å»ºæœç´¢URL
            search_url = f"{self.base_url}/search/results/people/?keywords={keywords}"
            if location:
                search_url += f"&location={location}"

            print(f"ğŸ” æ­£åœ¨æœç´¢LinkedIn: {keywords}")
            self.driver.get(search_url)
            self.random_sleep(3, 5)

            # æ»šåŠ¨åŠ è½½æ›´å¤š
            for _ in range(3):
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                self.random_sleep(2, 3)

            # æŸ¥æ‰¾äººå‘˜å¡ç‰‡
            person_elements = self.driver.find_elements(By.CSS_SELECTOR, ".entity-result, .reusable-search__result-container")

            print(f"ğŸ‘¥ æ‰¾åˆ° {len(person_elements)} ä¸ªç”¨æˆ·")

            for idx, person in enumerate(person_elements[:limit]):
                try:
                    # æå–å§“å
                    name_elem = person.find_element(By.CSS_SELECTOR, ".entity-result__title-text a, .app-aware-link")
                    name = name_elem.text.strip()
                    profile_url = name_elem.get_attribute("href")

                    # æå–èŒä½
                    try:
                        title_elem = person.find_element(By.CSS_SELECTOR, ".entity-result__primary-subtitle")
                        title = title_elem.text.strip()
                    except:
                        title = ""

                    # æå–å…¬å¸
                    try:
                        company_elem = person.find_element(By.CSS_SELECTOR, ".entity-result__secondary-subtitle")
                        company = company_elem.text.strip()
                    except:
                        company = ""

                    # æå–åœ°åŒº
                    try:
                        location_elem = person.find_element(By.CSS_SELECTOR, ".entity-result__location")
                        loc = location_elem.text.strip()
                    except:
                        loc = location

                    results.append({
                        'name': name,
                        'title': title,
                        'company': company,
                        'location': loc,
                        'profile_url': profile_url,
                        'source': 'linkedin',
                        'keywords': keywords,
                        'scraped_at': datetime.now().isoformat()
                    })

                    print(f"  âœ… [{idx+1}] {name} - {title} @ {company}")

                except Exception as e:
                    print(f"  âš ï¸ è§£æç”¨æˆ·å¤±è´¥: {e}")
                    continue

            print(f"âœ… æˆåŠŸæŠ“å– {len(results)} ä¸ªLinkedInç”¨æˆ·")

        except Exception as e:
            print(f"âŒ LinkedInæŠ“å–å¤±è´¥: {e}")

        return results


class ZhihuScraper(BaseScraper):
    """çŸ¥ä¹çœŸå®æŠ“å–"""

    def __init__(self, headless: bool = True):
        super().__init__(headless)
        self.base_url = "https://www.zhihu.com"

    def search_questions(self, keywords: str, limit: int = 20) -> List[Dict]:
        """
        æœç´¢çŸ¥ä¹é—®é¢˜

        Args:
            keywords: æœç´¢å…³é”®è¯(ä¾‹å¦‚: "ç¾å›½ç•™å­¦")
            limit: è¿”å›æ•°é‡

        Returns:
            List[Dict]: é—®é¢˜åˆ—è¡¨
        """
        if not self.driver:
            self.init_driver()

        results = []

        try:
            # è®¿é—®æœç´¢é¡µé¢
            search_url = f"{self.base_url}/search?type=content&q={keywords}"
            print(f"ğŸ” æ­£åœ¨æœç´¢çŸ¥ä¹: {keywords}")
            self.driver.get(search_url)
            self.random_sleep(3, 5)

            # æ»šåŠ¨åŠ è½½
            for _ in range(3):
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                self.random_sleep(2, 3)

            # æŸ¥æ‰¾é—®é¢˜å¡ç‰‡
            question_elements = self.driver.find_elements(By.CSS_SELECTOR, ".List-item, .SearchResult-Card")

            print(f"â“ æ‰¾åˆ° {len(question_elements)} ä¸ªé—®é¢˜")

            for idx, question in enumerate(question_elements[:limit]):
                try:
                    # æå–æ ‡é¢˜
                    title_elem = question.find_element(By.CSS_SELECTOR, ".ContentItem-title a, h2 a")
                    title = title_elem.text.strip()
                    question_url = title_elem.get_attribute("href")

                    # æå–æ‘˜è¦
                    try:
                        summary_elem = question.find_element(By.CSS_SELECTOR, ".RichContent-inner, .SearchItem-meta")
                        summary = summary_elem.text.strip()[:200]
                    except:
                        summary = ""

                    # æå–äº’åŠ¨æ•°æ®
                    try:
                        meta_elem = question.find_element(By.CSS_SELECTOR, ".ContentItem-meta")
                        meta_text = meta_elem.text
                        # æå–å…³æ³¨æ•°å’Œå›ç­”æ•°
                        followers = re.search(r'(\d+)\s*å…³æ³¨', meta_text)
                        answers = re.search(r'(\d+)\s*å›ç­”', meta_text)

                        follower_count = int(followers.group(1)) if followers else 0
                        answer_count = int(answers.group(1)) if answers else 0
                    except:
                        follower_count = 0
                        answer_count = 0

                    results.append({
                        'title': title,
                        'summary': summary,
                        'url': question_url,
                        'follower_count': follower_count,
                        'answer_count': answer_count,
                        'source': 'zhihu',
                        'keywords': keywords,
                        'scraped_at': datetime.now().isoformat()
                    })

                    print(f"  âœ… [{idx+1}] {title[:40]}... ({answer_count}å›ç­”)")

                except Exception as e:
                    print(f"  âš ï¸ è§£æé—®é¢˜å¤±è´¥: {e}")
                    continue

            print(f"âœ… æˆåŠŸæŠ“å– {len(results)} ä¸ªçŸ¥ä¹é—®é¢˜")

        except Exception as e:
            print(f"âŒ çŸ¥ä¹æŠ“å–å¤±è´¥: {e}")

        return results


class MultiPlatformScraper:
    """å¤šå¹³å°çœŸå®æŠ“å–èšåˆå™¨"""

    def __init__(self, linkedin_email: str = "", linkedin_password: str = "", headless: bool = True):
        self.linkedin_email = linkedin_email
        self.linkedin_password = linkedin_password
        self.headless = headless

    def scrape_all(self, keywords: str, platforms: List[str] = None, limit: int = 20) -> Dict:
        """
        åœ¨æ‰€æœ‰å¹³å°æŠ“å–æ•°æ®

        Args:
            keywords: æœç´¢å…³é”®è¯
            platforms: å¹³å°åˆ—è¡¨ ['linkedin', 'xiaohongshu', 'zhihu']
            limit: æ¯ä¸ªå¹³å°è¿”å›æ•°é‡

        Returns:
            Dict: å„å¹³å°ç»“æœ
        """
        if platforms is None:
            platforms = ['xiaohongshu', 'zhihu']  # é»˜è®¤ä¸åŒ…å«LinkedIn(éœ€è¦ç™»å½•)

        results = {
            'keywords': keywords,
            'timestamp': datetime.now().isoformat(),
            'platforms': {}
        }

        # å°çº¢ä¹¦
        if 'xiaohongshu' in platforms:
            print("\n" + "="*50)
            print("ğŸ“± å¼€å§‹æŠ“å–å°çº¢ä¹¦")
            print("="*50)
            try:
                with XiaohongshuScraper(headless=self.headless) as scraper:
                    results['platforms']['xiaohongshu'] = scraper.search_notes(keywords, limit)
            except Exception as e:
                print(f"âŒ å°çº¢ä¹¦æŠ“å–å¤±è´¥: {e}")
                results['platforms']['xiaohongshu'] = []

        # çŸ¥ä¹
        if 'zhihu' in platforms:
            print("\n" + "="*50)
            print("ğŸ“š å¼€å§‹æŠ“å–çŸ¥ä¹")
            print("="*50)
            try:
                with ZhihuScraper(headless=self.headless) as scraper:
                    results['platforms']['zhihu'] = scraper.search_questions(keywords, limit)
            except Exception as e:
                print(f"âŒ çŸ¥ä¹æŠ“å–å¤±è´¥: {e}")
                results['platforms']['zhihu'] = []

        # LinkedIn
        if 'linkedin' in platforms:
            print("\n" + "="*50)
            print("ğŸ’¼ å¼€å§‹æŠ“å–LinkedIn")
            print("="*50)
            try:
                with LinkedInScraper(self.linkedin_email, self.linkedin_password, headless=self.headless) as scraper:
                    results['platforms']['linkedin'] = scraper.search_people(keywords, limit=limit)
            except Exception as e:
                print(f"âŒ LinkedInæŠ“å–å¤±è´¥: {e}")
                results['platforms']['linkedin'] = []

        # ç»Ÿè®¡
        total_count = sum(len(v) if isinstance(v, list) else 0 for v in results['platforms'].values())
        print("\n" + "="*50)
        print(f"âœ… æŠ“å–å®Œæˆ! å…±è·å– {total_count} æ¡æ•°æ®")
        print("="*50)

        return results

    def convert_to_leads(self, scrape_results: Dict) -> List[Dict]:
        """
        å°†æŠ“å–ç»“æœè½¬æ¢ä¸ºçº¿ç´¢æ ¼å¼

        Args:
            scrape_results: æŠ“å–ç»“æœ

        Returns:
            List[Dict]: çº¿ç´¢åˆ—è¡¨
        """
        leads = []

        for platform, data in scrape_results.get('platforms', {}).items():
            if not isinstance(data, list):
                continue

            for item in data:
                lead = {
                    'name': '',
                    'email': '',
                    'phone': '',
                    'target_country': '',
                    'target_degree': '',
                    'major': '',
                    'budget': '',
                    'source': platform,
                    'source_url': item.get('url', ''),
                    'status': 'new',
                    'scraped_at': item.get('scraped_at', ''),
                    'notes': ''
                }

                if platform == 'linkedin':
                    lead['name'] = item.get('name', '')
                    lead['notes'] = f"èŒä½: {item.get('title', '')}\nå…¬å¸: {item.get('company', '')}\nåœ°åŒº: {item.get('location', '')}"

                elif platform == 'xiaohongshu':
                    lead['name'] = item.get('author', '')
                    lead['notes'] = f"ç¬”è®°: {item.get('title', '')}\nç‚¹èµ: {item.get('likes', 0)}"

                elif platform == 'zhihu':
                    lead['name'] = 'çŸ¥ä¹ç”¨æˆ·'
                    lead['notes'] = f"é—®é¢˜: {item.get('title', '')}\nå›ç­”æ•°: {item.get('answer_count', 0)}\nå…³æ³¨æ•°: {item.get('follower_count', 0)}"

                leads.append(lead)

        return leads


# æµ‹è¯•å‡½æ•°
def test_scraper():
    """æµ‹è¯•æŠ“å–åŠŸèƒ½"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•çœŸå®æŠ“å–...")

    scraper = MultiPlatformScraper(headless=False)  # æ˜¾ç¤ºæµè§ˆå™¨æ–¹ä¾¿è°ƒè¯•

    # æµ‹è¯•æŠ“å–
    results = scraper.scrape_all(
        keywords="ç¾å›½ç•™å­¦",
        platforms=['xiaohongshu', 'zhihu'],  # å…ˆæµ‹è¯•è¿™ä¸¤ä¸ª,LinkedInéœ€è¦ç™»å½•
        limit=5
    )

    # è½¬æ¢ä¸ºçº¿ç´¢
    leads = scraper.convert_to_leads(results)

    print(f"\nâœ… å…±è½¬æ¢ {len(leads)} æ¡çº¿ç´¢")

    # ä¿å­˜ç»“æœ
    with open('scrape_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print("ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° scrape_results.json")

    return results, leads


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    if SELENIUM_AVAILABLE:
        test_scraper()
    else:
        print("âŒ è¯·å…ˆå®‰è£…ä¾èµ–: pip install selenium undetected-chromedriver")
